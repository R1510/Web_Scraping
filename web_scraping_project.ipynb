{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import requests\n",
        "!pip install bs4 --upgrade --quite\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def getdata(url):\n",
        "  r=requests.get(url)\n",
        "  print(r.text)\n",
        "  return r.text\n",
        "#headers={'User-Agent':'Mozilla / 5.0 (Linux; Android 6.0.1; Nexus 5X Build / MMB29P) AppleWebKit / 537.36 (KHTML, like Gecko) Chrome / 41.0.2272.96 Mobile Safari / 537.36 (compatible; Googlebot / 2.1; + http: //www.google .com / bot.html)'}\n",
        "url='https://in.indeed.com/?from=gnav-homepage'\n",
        "data=getdata(url)\n",
        "soup=BeautifulSoup(data,'html.parser')\n",
        "def job_title(job_ele):\n",
        "  title_ele=job_ele.find('h2',class_='screenreader')\n",
        "def extract_company(job_ele):\n",
        "  company_ele=job_ele.find('div',class_='jobsearch-InlineCompanyRating-companyHeader')\n",
        "  company=company_ele.text.strip()\n",
        "  return company\n",
        "def job_salary(job_ele):\n",
        "  salary_ele=job_ele.find('span',class_='icl-u-xs-mr--xs')\n",
        "  salary=salary_ele.text\n",
        "  return salary\n",
        "def job_qualifications(job_ele):\n",
        "  qualifications_ele=job_ele.find('div',class_='jobsearch-ReqAndQualSection-item--wrapper')\n",
        "  qualifications=qualifications_ele.text.strip()\n",
        "  return qualifications\n",
        "def extract_link(job_ele):\n",
        "  link=job_ele.find('a')['href']\n",
        "  return link\n",
        "def job_data(soup):\n",
        "  data_st=\" \"\n",
        "  job_ele= soup.find_all(\"div\",class_=\"jobsearch \")\n",
        "  cols=[]\n",
        "  extracted_info=[]\n",
        "  desired_characs={titles,companies,salary_details,qualification_details,links}\n",
        "  if 'titles' in desired_characs:\n",
        "    titles = []\n",
        "    cols.append('titles')\n",
        "    for job_elem in job_ele:\n",
        "        titles.append(job_title(job_elem))\n",
        "    extracted_info.append(titles)\n",
        "  #job_title(job_ele)\n",
        "  if 'companies' in desired_characs:\n",
        "    companies = []\n",
        "    cols.append('companies')\n",
        "    for job_elem in job_ele:\n",
        "        companies.append(extract_company(job_elem))\n",
        "    extracted_info.append(companies)\n",
        "  #extract_company(job_ele)\n",
        "  if 'salary_datails' in desired_characs:\n",
        "    salary_details=[]\n",
        "    cols.append('salary_details')\n",
        "    for job_elem in job_ele:\n",
        "      salary_details.append(job_salary(job_elem))\n",
        "    extracted_info.append(salary_details)\n",
        "  #job_salary(job_ele)\n",
        "  if 'qualification_details' in desired_characs:\n",
        "    qualification_details=[]\n",
        "    cols.append('qualification_details')\n",
        "    for job_elem in job_ele:\n",
        "      qualification_details.append(job_qualifications(job_elem))\n",
        "    extracted_info.append(qualification_details)\n",
        "  #job_qualifications(job_ele)\n",
        "  if 'links' in desired_characs:\n",
        "    links = []\n",
        "    cols.append('links')\n",
        "    for job_elem in job_ele:\n",
        "        links.append(extract_link(job_elem))\n",
        "    extracted_info.append(links)\n",
        "  \n",
        "  jobs_list = {}\n",
        "  for j in range(len(cols)):\n",
        "    jobs_list[cols[j]] = extracted_info[j]\n",
        "    filename=\"results.xls\";\n",
        "  save_jobs_to_excel(jobs_list,filename)\n",
        "  #extract_link(job_ele)\n",
        "#print(job_data(soup))\n",
        "def save_jobs_to_excel(jobs_list, filename):\n",
        "    jobs = pd.DataFrame(jobs_list)\n",
        "    jobs.to_excel(filename)\n",
        "    num_listings = len(extracted_info[0])\n",
        "    \n",
        "    return jobs_list, num_listings\n",
        "print('{} new job postings retrieved. Stored in {}.'.format(num_listings, filename))\n",
        "#def company_data(soup):\n",
        "  \n",
        "    # find the Html tag\n",
        "    # with find()\n",
        "    # and convert into string\n",
        " #   data_str = \"\"\n",
        "  #  result = \"\"\n",
        "   # for item in soup.find_all(\"div\", class_=\"jobsearch\"):\n",
        "    #    data_str = data_str + item.get_text()\n",
        "    #result_1 = data_str.split(\"\\n\")\n",
        "  \n",
        "    #res = []\n",
        "    #for i in range(1, len(result_1)):\n",
        "     #   if len(result_1[i]) > 1:\n",
        "      #      res.append(result_1[i])\n",
        "    #return(res)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "PKf_it5T8fHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#CODE OF WEBSCRPING WITH PYTHON:\n",
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "def getdata(job):\n",
        "\n",
        "  job_title=job.a.text.strip()\n",
        "  job_url=job.a.get('href')\n",
        "  posted_time=job.find('div',class_=\"job-age\").text.strip()\n",
        "  job_details=job.find('div',class_=\"row align-items-center mb-2\")\n",
        "  job_place=job.find('div',class_=\"col pe-0 job-locations text-truncate\").text.strip()\n",
        "  job_desc=job.find('div',class_=\"job-description\")\n",
        "#   print(job_title)\n",
        "  #print(job_url)\n",
        "  #print(posted_time)\n",
        "  #print(job_desc)\n",
        "  #print(job_place)\n",
        "  information=[job_title,posted_time,job_desc.get_text(),job_place,\"https://www.flexjobs.com\"+job_url]\n",
        "\n",
        "  return information\n",
        "\n",
        "def main():\n",
        "\n",
        "    total_information=[]\n",
        "    url='https://www.flexjobs.com/search?search=work+from+home+part+time&location='\n",
        "    # print(len(jobs))\n",
        "#   while True:\n",
        "    r=requests.get(url)\n",
        "\n",
        "    soup=BeautifulSoup(r.text,'html.parser')\n",
        "    jobs=soup.find_all('div',class_=\"col-md-12 col-12\")\n",
        "    for job in jobs:\n",
        "        information=getdata(job)\n",
        "        total_information.append(information)\n",
        "    with open('results.csv','w',encoding='utf-8')as f:\n",
        "        writer=csv.writer(f)\n",
        "        writer.writerow(['job_title','posted_time','job_desc','job_place','job_url'])\n",
        "        writer.writerows(total_information)\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "PgwvZ1dVGrwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}